{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"x-Xm_Qh93JdR"},"outputs":[],"source":["import sys\n","import os\n","import time\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import r2_score\n","import matplotlib.pyplot as plt\n","import torch.optim.lr_scheduler as lr_scheduler\n","from torch.optim.lr_scheduler import CyclicLR\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import math\n","from torch.utils.data import TensorDataset, DataLoader\n","from tqdm import tqdm_notebook\n","from pathlib import Path\n","import random\n","import torch.nn.init as init\n","import openpyxl\n","\n","from datetime import timedelta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oM3R1mZkowpk"},"outputs":[],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# import os\n","\n","# google_drive_path = \"G:/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVjLjygN_UFK"},"outputs":[],"source":["torch.manual_seed(42)\n","random.seed(42)\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtofuMh-kCkc"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","    print('and then re-execute this cell.')\n","else:\n","    print(gpu_info)\n","\n","CUDA = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if CUDA else \"cpu\")\n","\n","# torch and python versions ====================================================\n","print('torch.__version__:',torch.__version__)\n","print('sys.version:',sys.version)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aobf1S6lTIne"},"outputs":[],"source":["DATASET_NAMES = ['Site3']\n","VARIABLE_NAMES = ['Temperature', 'V', 'Sin(dir)','Cos(dir)']\n","UNITS_OF_MEASUREMENT = ['°C', 'm/s', '', '']\n","POSITIVE_VARIABLES = [1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ckN1DBAh-ovk"},"outputs":[],"source":["# Hyperparameters ==============================================================\n","LOOK_FORWARD_DAYS_LIST = [1,2,3]\n","DATASETS_TO_PROCESS = [0]\n","VARIABLES_TO_PROCESS = [1,2,3]\n","\n","LOOK_BACK_DAYS = 3\n","TEST_FRACTION = 0.2\n","VALIDATION_FRACTION = 0.2\n","PATIENCE = 10\n","SHUFFLE = False\n","FORCE_TRAINING = False\n","DEBUG = False\n","USE_ADAM = True\n","USE_EARLY_STOP = True\n","USE_VALIDATION = True\n","DAYS_TO_PLOT = 3\n","POST_PROCESS = True\n","PRINT_RMSE = True\n","\n","STANDARD_SCALERS = False\n","PLOT_DISTRIBUTIONS = False\n","OUTLIERS = 0.25\n","# ==============================================================================\n","\n","if not USE_VALIDATION:\n","  VALIDATION_FRACTION = 0.0\n","\n","HP = {0: # Site 3\n","         {0:  { 'HIDDEN_DIM' : 128,\n","                'N_LAYERS' : 2,\n","                'BATCH_SIZE' : 512,\n","                'LEARNING_RATE' : 0.001,\n","                'TEST_PLOT_START' : [9035, 8920, 8800],\n","                'EPOCHS' : 200},\n","\n","          1:  { 'HIDDEN_DIM' :128,\n","                'N_LAYERS' : 2,\n","                'BATCH_SIZE' : 512,\n","                'LEARNING_RATE' : 0.001,\n","                'TEST_PLOT_START' : [9035, 8920, 8800],\n","                'EPOCHS' : 200},\n","\n","          2:  { 'HIDDEN_DIM' : 128,\n","                'N_LAYERS' : 2, #4\n","                'BATCH_SIZE' : 512, #256\n","                'LEARNING_RATE' : 0.001,  #0.0001\n","                'TEST_PLOT_START' : [9035, 8920, 8800],\n","                'EPOCHS' : 200},\n","\n","          3:  { 'HIDDEN_DIM' : 128,\n","                'N_LAYERS' : 2, #4\n","                'BATCH_SIZE' : 512, #256\n","                'LEARNING_RATE' : 0.001,  #0.0001\n","                'TEST_PLOT_START' : [9035, 8920, 8800],\n","                'EPOCHS' : 200}\n","\n","        }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ss8RHV4ek2-n"},"outputs":[],"source":["# Loop through each dataset in DATASETS_TO_PROCESS\n","for dataset_id in DATASETS_TO_PROCESS:\n","  # Access the configuration dictionary for the current dataset\n","  dataset_config = HP[dataset_id]\n","\n","  # Loop through each variable configuration within the dataset\n","  for variable_id in VARIABLES_TO_PROCESS:\n","    # Access the 'HIDDEN_DIM' value for the current variable\n","    hidden_dim = dataset_config[variable_id]['HIDDEN_DIM']\n","  for variable_id in VARIABLES_TO_PROCESS:\n","    # Access the 'N_LAYERS' value for the current variable\n","    n_layers = dataset_config[variable_id]['N_LAYERS']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxSfdwoXhkU1"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, input_dim, batch_size, learning_rate, hidden_dim, output_dim, n_layers, positive_variable, drop_prob=0.15):\n","        super(Model, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.positive_variable=positive_variable\n","\n","        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","        #self.bat_a = nn.BatchNorm1d(hidden_dim)\n","        #self.bat_b = nn.BatchNorm1d(hidden_dim)\n","        self.activation_a = nn.ReLU()\n","        #self.activation_b = nn.ReLU()\n","        self.fc_a = nn.Linear(hidden_dim, hidden_dim//2)\n","        self.fc_b = nn.Linear(hidden_dim//2, output_dim)\n","\n","    def forward(self, x, h):\n","        out, h = self.gru(x, h)\n","        # print('out.shape (before):',out.shape)\n","\n","        # The following statement is important: we are implementing a S2V model,\n","        # so we select the latest output.\n","\n","        out=out[:,-1]\n","        # print('out.shape (after):',out.shape)\n","\n","\n","        out=self.fc_a(out)\n","        out=self.activation_a(out)\n","        out = self.dropout(out)\n","        out=self.fc_b(out)\n","        if self.positive_variable:\n","          #out=torch.exp(out)\n","          #out=torch.abs(out)\n","          #out=torch.square(out)\n","          pass\n","\n","        return out, h\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n","        return hidden"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OfuAA_A_mXM"},"outputs":[],"source":["def process_epoch(model, data_loader, criterion, optimizer, train, variable):\n","    n_layers = model.n_layers\n","    hidden_dim = model.hidden_dim\n","    batch_size = data_loader.batch_size\n","\n","    if SHUFFLE:\n","        h = torch.zeros([n_layers, batch_size, hidden_dim]).to(device)\n","    else:\n","        h = model.init_hidden(batch_size)\n","\n","    total_loss = 0.\n","    counter = 0\n","\n","    for x, y, local_means in data_loader:\n","        counter += 1\n","\n","        if SHUFFLE:\n","            h = torch.zeros([n_layers, batch_size, hidden_dim]).to(device)\n","        else:\n","            h = h.data\n","\n","        model.zero_grad()\n","\n","\n","        y_hat, h = model(x.to(device).float(), h)\n","        base_loss = criterion(y_hat, y.to(device).float())\n","\n","        #local_mean_loss = torch.mean(torch.abs(y_hat - local_means.to(device).float()))  # l1\n","        #local_mean_loss = torch.mean((y_hat - local_means.to(device).float()) ** 2) # mse\n","        local_mean_loss = criterion(y_hat, local_means.to(device).float())\n","\n","        α = 0.5\n","        loss = α * base_loss + (1-α) * local_mean_loss\n","        #loss = local_mean_loss\n","        #loss = base_loss\n","\n","        total_loss += loss.item()\n","\n","        if train:\n","            loss.backward()\n","            optimizer.step()\n","\n","        # if counter%20==0:\n","        #     print(\"Step: {}/{}, Average Loss: {}, Local Mean Loss: {}\".format(\n","        #         counter, len(data_loader), total_loss/counter, local_mean_loss.item()))\n","\n","        if counter%20==0:\n","            print(\"Step: {}/{}, Average Loss: {}\".format(\n","                counter, len(data_loader), total_loss/counter))\n","\n","\n","    avg_loss = total_loss/len(data_loader)\n","    return avg_loss\n","\n","def train(model_name, train_loader, validation_loader, learning_rate, batch_size, positive_variable=False,\n","          hidden_dim = hidden_dim, n_layers = n_layers, patience=None, epochs=None):\n","    input_dim = next(iter(train_loader))[0].shape[2]\n","    output_dim = 1\n","\n","    print('Input dim.:', input_dim)\n","    print('Output dim.:', output_dim)\n","\n","    # The model\n","    model = Model(input_dim, hidden_dim=hidden_dim, output_dim=1,batch_size=batch_size, learning_rate=learning_rate, n_layers=n_layers, positive_variable=positive_variable)\n","    model.to(device)\n","\n","    # Loss function and optimizer\n","    #criterion = nn.MSELoss()\n","    criterion = nn.L1Loss()\n","\n","    if USE_ADAM:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    else:\n","      optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n","\n","    model.train()\n","\n","    epoch=1\n","    patience_counter = 0\n","    min_validation_loss=None\n","    validation_loss=None\n","\n","    train_loss_list=[]\n","    validation_loss_list=[]\n","\n","    # Training loop ============================================================\n","    while True:\n","        print(\"Epoch {}\".format(epoch))\n","\n","        print('Training...')\n","        train_loss = process_epoch(model, train_loader, criterion, optimizer, train = True, variable=variable)\n","\n","        if USE_VALIDATION:\n","          print('Validation...')\n","          validation_loss = process_epoch(model, validation_loader, criterion, optimizer, train = False, variable=variable)\n","          validation_loss_list.append(validation_loss)\n","\n","        train_loss_list.append(train_loss)\n","\n","        if min_validation_loss is None or validation_loss<min_validation_loss or not USE_VALIDATION:\n","          # Save the model\n","          print('Saving the model...')\n","          torch.save(model, model_name)\n","\n","          min_validation_loss = validation_loss\n","          patience_counter = 0\n","        else:\n","          print('Increasing patience counter')\n","          patience_counter +=1\n","\n","        print(\"Epoch {} Completed, Train Loss: {}, Validation Loss: {}, Patience counter: {}\".format(epoch, train_loss, validation_loss, patience_counter))\n","\n","        if (patience_counter>patience and USE_EARLY_STOP) or epoch==epochs:\n","          break\n","\n","        epoch += 1\n","\n","    print('Restoring the best model...')\n","    model = torch.load(model_name, weights_only=False)\n","    return model, train_loss_list, validation_loss_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtcsBXq9PYJw"},"outputs":[],"source":["def evaluate(model, test_x, test_y, label_scaler, post_process, mm5_array):\n","    outputs = []\n","    targets = []\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    for i in range(test_x.shape[0]):\n","        inp = torch.from_numpy(np.expand_dims(test_x[i], axis=0))\n","\n","        out, h = model(inp.to(device).float(), None)\n","\n","        outputs.append(out[0,0].item())\n","        targets.append(test_y[i])\n","\n","        if i%5000==0:\n","            print(i,'/',test_x.shape[0],sep='')\n","\n","    predictions_array = label_scaler.inverse_transform(np.array(outputs).reshape(-1, 1))\n","    targets_array = label_scaler.inverse_transform(np.array(targets).reshape(-1, 1))\n","\n","    if post_process:\n","      predictions_array = predictions_array * (mm5_array!=0)\n","\n","    return predictions_array, targets_array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmI28SVo89z9"},"outputs":[],"source":["def se_array(x, y):\n","  return np.square(x-y)\n","\n","def ae_array(x, y):\n","  return np.abs(x - y)\n","\n","def rmse(x,y):\n","  return np.sqrt(mean_squared_error(x,y))\n","\n","def mae(x, y):\n","    return np.mean(np.abs(x-y))\n","\n","def latex_row(a,b,c,bigger):\n","  if a>b and bigger or a<b and not bigger:\n","    return '\\\\textbf{'+str(round(a, 3))+'}\\\\\\\\ '+str(round(b, 3))+'\\\\\\\\ '+str(round(c, 2))+'\\%'\n","  else:\n","    return str(round(a, 3))+'\\\\\\\\ \\\\textbf{'+str(round(b, 3))+'}\\\\\\\\ '+str(round(c, 2))+'\\%'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SM3GPtXlRREr"},"outputs":[],"source":["def calculate_local_mean(values, window_size=3):\n","    result = np.zeros_like(values)\n","    padded = np.pad(values, (window_size, window_size), mode='edge')\n","\n","    for i in range(len(values)):\n","        window_start = i\n","        window_end = i + 2 * window_size + 1\n","        result[i] = np.mean(padded[window_start:window_end])\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnG6GdbpFWdy"},"outputs":[],"source":["for LOOK_FORWARD_DAYS in LOOK_FORWARD_DAYS_LIST:\n","  for dataset in DATASETS_TO_PROCESS:\n","      for variable in [2,3]:  # VARIABLES_TO_PROCESS:\n","        print('='*80)\n","        print('Look Forward Days:', LOOK_FORWARD_DAYS)\n","        print('Dataset:', DATASET_NAMES[dataset])\n","        print('Variable:', VARIABLE_NAMES[variable])\n","        print('_'*80)\n","\n","        if variable not in HP[dataset].keys():\n","          continue\n","\n","        data_root = '/content/drive/MyDrive/Colab Notebooks/datasets/energy/'\n","        root = data_root + 'results/'\n","\n","        suffix = '_look_forward_days_' + str(LOOK_FORWARD_DAYS) + '_dataset_' + str(dataset) + '_variable_' + str(variable)\n","\n","        report_name = root + 'report'+ suffix + '.txt'\n","        report_file = open(report_name, \"w\")\n","\n","        report_file.write('Dataset {}, Variable {}\\n'.format(str(dataset), VARIABLE_NAMES[variable]))\n","        report_file.write('_'*80+'\\n')\n","\n","        # Create dataframe =======================================================\n","\n","        # ============================================================================\n","        # DATA LOADING\n","        # ============================================================================\n","        # NOTE: Due to confidentiality agreements, the datasets cannot be publicly\n","        # shared. The code is provided for transparency and reproducibility of the\n","        # methodology. Researchers with similar datasets can adapt this code for\n","        # their own data.\n","        #\n","        # Expected file structure in Google Drive:\n","        # MyDrive/Colab Notebooks/datasets/energy/dataset_{0,1}_look_forward_days_{1,2,3}.xlsx\n","        # ============================================================================\n","\n","        url = data_root + 'dataset_'+str(dataset)+'_look_forward_days_' + str(LOOK_FORWARD_DAYS)+'.xlsx'\n","        df = pd.read_excel(url)\n","\n","        pd.set_option('display.float_format', lambda x: '%.2f' % x)\n","        #print(df.head)\n","\n","        df['dayofyear'] = df.apply(lambda x: x['Data_R'].dayofyear,axis=1)\n","        df['hour'] = df.apply(lambda x: x['Ora_R'].hour,axis=1)\n","        #df['dayofweek'] = df.apply(lambda x: x['Data_R'].dayofweek,axis=1)\n","        #df['month'] = df.apply(lambda x: x['Data_R'].month,axis=1)\n","        #df['minute'] = df.apply(lambda x: x['Ora_R'].minute,axis=1)\n","        #df['DateTime'] = pd.to_datetime(df['Data_R'].astype(str) + ' ' + df['Ora_R'].astype(str))\n","\n","        datetime_info = df[['Data_R', 'Ora_R']].copy()\n","\n","        df = df.drop([\"Data_R\", \"Ora_R\", \"Data_MM5\", \"Ora_MM5\"], axis=1)\n","\n","\n","        df = df.replace(\"None\", np.nan)\n","\n","        if PLOT_DISTRIBUTIONS:\n","            df[df['GHI_R'].notna()]['GHI_R'].plot.hist(bins=100)\n","            plt.pause(0.001)\n","            df[df['Temperatura_R'].notna()]['Temperatura_R'].plot.hist(bins=100)\n","            plt.pause(0.001)\n","            df[df['V_Tot_R [30m]'].notna()]['V_R'].plot.hist(bins=100)\n","            plt.pause(0.001)\n","            df[df['Sin_R'].notna()]['Sin_R'].plot.hist(bins=100)\n","            plt.pause(0.001)\n","            df[df['Cos_R'].notna()]['Cos_R'].plot.hist(bins=100)\n","            plt.pause(0.001)\n","            break\n","\n","        NUMBER_OF_VARIABLES = (len(df.columns)-2)//2\n","        print('Number of variables:', NUMBER_OF_VARIABLES)\n","\n","        # Select current hyperparameters =========================================\n","        HIDDEN_DIM = HP[dataset][variable]['HIDDEN_DIM']\n","        N_LAYERS = HP[dataset][variable]['N_LAYERS']\n","        BATCH_SIZE = HP[dataset][variable]['BATCH_SIZE']\n","        LEARNING_RATE = HP[dataset][variable]['LEARNING_RATE']\n","        TEST_PLOT_START = HP[dataset][variable]['TEST_PLOT_START'][LOOK_FORWARD_DAYS-1]\n","        TEST_PLOT_STOP = TEST_PLOT_START+144*DAYS_TO_PLOT\n","        EPOCHS = HP[dataset][variable]['EPOCHS']\n","\n","        for hp in HP[dataset][variable].keys():\n","            print(hp,': ', HP[dataset][variable][hp],sep='')\n","            pass\n","\n","        # Create scalers =========================================================\n","        data = df.select_dtypes(include=['number', 'float', 'int']).astype('float32').to_numpy()\n","        print(type(data))\n","\n","        if STANDARD_SCALERS:\n","          scaler = StandardScaler()\n","          label_scaler = StandardScaler()\n","          mm5_scaler = StandardScaler()\n","        else:\n","          scaler = MinMaxScaler()\n","          label_scaler = MinMaxScaler()\n","          mm5_scaler = MinMaxScaler()\n","\n","        if dataset==0 or variable==0:\n","          label_scaler.fit(data[:,variable].reshape(-1,1))\n","          mm5_scaler.fit(data[:,variable+NUMBER_OF_VARIABLES].reshape(-1,1))\n","\n","        else:\n","          label_scaler.fit(data[:,variable].reshape(-1,1))\n","          mm5_scaler.fit(data[:,variable+NUMBER_OF_VARIABLES].reshape(-1,1))\n","\n","        data = scaler.fit_transform(data)\n","\n","        if DEBUG:\n","            print(scaler.data_min_)\n","            print(scaler.data_max_)\n","            print(label_scaler.data_min_)\n","            print(label_scaler.data_max_)\n","            print(mm5_scaler.data_min_)\n","            print(mm5_scaler.data_max_)\n","\n","\n","        # Define lookback period and split inputs/labels =========================\n","        look_back_steps = 144 * LOOK_BACK_DAYS\n","        look_forward_steps = 144 * LOOK_FORWARD_DAYS\n","\n","        inputs = np.zeros((len(data)-look_back_steps-look_forward_steps+1,look_back_steps,NUMBER_OF_VARIABLES+2))\n","        local_means = np.zeros(len(data)-look_back_steps-look_forward_steps+1)\n","        labels = np.zeros(len(data)-look_back_steps-look_forward_steps+1)\n","        dates = np.zeros(len(data)-look_back_steps-look_forward_steps+1, dtype='object')\n","        hours = np.zeros(len(data)-look_back_steps-look_forward_steps+1, dtype='object')\n","\n","        if DEBUG:\n","            print(data.shape)\n","            print(inputs.shape)\n","            print(labels.shape)\n","\n","        mask=np.full(inputs.shape[0], True)\n","\n","        # ONLY MM5 DATA:\n","\n","        # The input is a sequence of vectors containing the MM5 values from time\n","        # 'T-look_back_steps+1' to time 'T' (where T is future instant of time the\n","        # prediction is referred to)\n","        # Try look_back_steps=look_forward_steps=1\n","\n","        # The models takes in input only MM5 values that cannot be null\n","        # That's why some data rows previously deleted now are included and the\n","        # MM5 performance metrics are slightly different.\n","\n","        for i in range(look_back_steps, len(data) - look_forward_steps + 1):\n","\n","            index = i-look_back_steps\n","\n","            inputs[index] = data[index : i, NUMBER_OF_VARIABLES:] # MM5\n","            # inputs[index] = data[index : i]\n","\n","            labels[index] = data[i + look_forward_steps - 1, variable]\n","\n","            dates[index] = datetime_info['Data_R'][i + look_forward_steps - 1]  # Inputs and labels are NumPy because of the scaler. datetime_info, on the other hand, is a pandas DataFrame, which has specific, different indexing methods.\n","            hours[index] = datetime_info['Ora_R'][i + look_forward_steps - 1]\n","\n","            if np.isnan(inputs[index]).any() or np.isnan(labels[index]).any():\n","                #print('input:',inputs[index])\n","                #print('label:',labels[index])\n","                mask[index] = False\n","\n","            if i%10000==0 and DEBUG:\n","                print(i)\n","                print('-'*80)\n","                print(inputs[index])\n","                print(labels[index])\n","\n","        # Removing tuples containing nan values\n","        if DEBUG or True:\n","            print('BEFORE')\n","            print(inputs.shape)\n","            print(labels.shape)\n","            print('nan inputs:', np.isnan(inputs).any())\n","            print('nan labels:', np.isnan(labels).any())\n","            print('Number of nan tuples:', np.sum(np.logical_not(mask)))\n","\n","        inputs=inputs[mask]\n","        labels=labels[mask]\n","        dates = dates[mask]\n","        hours = hours[mask]\n","\n","        if DEBUG or True:\n","            print('AFTER')\n","            print(inputs.shape)\n","            print(labels.shape)\n","            print('nan inputs:', np.isnan(inputs).any())\n","            print('nan labels:', np.isnan(labels).any())\n","\n","        # inputs = inputs.reshape(-1, look_back_steps, df.shape[1])\n","        inputs = inputs.reshape(-1, look_back_steps, NUMBER_OF_VARIABLES +2) # MM5\n","        labels = labels.reshape(-1, 1)\n","\n","        local_means = calculate_local_mean(labels.flatten())\n","        local_means = local_means.reshape(-1, 1)\n","\n","        # Split data into training set, validation set and test set\n","        validation_portion = int(VALIDATION_FRACTION*len(inputs))\n","        test_portion = int(TEST_FRACTION*len(inputs))\n","        validation_test_portion = validation_portion + test_portion\n","\n","        train_x = inputs[:-validation_test_portion]\n","        train_y = labels[:-validation_test_portion]\n","        train_local_means = local_means[:-validation_test_portion]\n","\n","        validation_x = inputs[-validation_test_portion:-test_portion]\n","        validation_y = labels[-validation_test_portion:-test_portion]\n","        validation_local_means = local_means[-validation_test_portion:-test_portion]\n","\n","        test_x = inputs[-test_portion:]\n","        test_y = labels[-test_portion:]\n","        test_dates = dates[-test_portion:]\n","        test_hours = hours[-test_portion:]\n","\n","\n","        # Create dataloaders =====================================================\n","        train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y), torch.from_numpy(train_local_means))\n","        train_loader = DataLoader(train_data, shuffle=SHUFFLE, batch_size=BATCH_SIZE, drop_last=True)\n","\n","        validation_data = TensorDataset(torch.from_numpy(validation_x), torch.from_numpy(validation_y), torch.from_numpy(validation_local_means))\n","        validation_loader = DataLoader(validation_data, shuffle=SHUFFLE, batch_size=BATCH_SIZE, drop_last=True)\n","\n","        train_loss_list=None\n","        validation_loss_list=None\n","\n","        # Create the model =======================================================\n","        model_name = root + 'model' + suffix + '.zip'\n","        print(model_name)\n","\n","        if Path(model_name).is_file() and not FORCE_TRAINING:\n","            model = torch.load(model_name, weights_only=False)\n","\n","        else:\n","            model, train_loss_list, validation_loss_list = train(model_name, train_loader, validation_loader, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE, hidden_dim=HIDDEN_DIM, n_layers=N_LAYERS, positive_variable=(variable in POSITIVE_VARIABLES), patience=PATIENCE, epochs=EPOCHS)\n","\n","        model.eval()\n","\n","        # Loss graphs ============================================================\n","        if train_loss_list is not None and validation_loss_list is not None:\n","          plt.figure(figsize=(15,10))\n","          plt.subplot(2,2,1)\n","          ma_domain=[x for x in range(2,len(validation_loss_list))]\n","          plt.plot(train_loss_list, color=\"navajowhite\", label=\"Train Loss\", linewidth=0.7)\n","          plt.plot(validation_loss_list, color=\"lightblue\", label=\"Validation Loss\", linestyle=\"dashed\", linewidth=0.7)\n","\n","          train_loss_ma=[(train_loss_list[i]+train_loss_list[i+1]+train_loss_list[i+2])/3 for i in range(len(train_loss_list)-2)]\n","          validation_loss_ma=[(validation_loss_list[i]+validation_loss_list[i+1]+validation_loss_list[i+2])/3 for i in range(len(validation_loss_list)-2)]\n","\n","          plt.plot(ma_domain, train_loss_ma, color=\"darkorange\", label=\"Train Loss (MA)\", linewidth=0.7)\n","          plt.plot(ma_domain, validation_loss_ma, color=\"blue\", label=\"Validation Loss (MA)\", linestyle=\"dashed\", linewidth=0.7)\n","\n","          plt.axvline(x = validation_loss_ma.index(min(validation_loss_ma)), color=\"r\", linewidth=0.7)\n","\n","          plt.ylabel('Loss')\n","          plt.xlabel('Epoch')\n","          plt.grid()\n","          plt.legend()\n","\n","          # Save the figure ========================================================\n","          figure_name = root + 'loss_figure'+suffix+'.png'\n","          plt.savefig(figure_name, dpi=(250), bbox_inches='tight')\n","\n","        if DEBUG:\n","            for name, param in model.named_parameters():\n","                if param.requires_grad:\n","                    print(name, param.data)\n","                    pass\n","\n","        # Evaluation =============================================================\n","\n","        if dataset==0 or variable==0:\n","          #mm5_array = mm5_scaler.inverse_transform(test_x[:, -1, variable + NUMBER_OF_VARIABLES].reshape(-1,1))\n","          mm5_array = mm5_scaler.inverse_transform(test_x[:, -1, variable].reshape(-1,1)) # MM5\n","        else:\n","          #mm5_array = mm5_scaler.inverse_transform(test_x[:, -1, variable + NUMBER_OF_VARIABLES].reshape(-1,1))\n","          mm5_array = mm5_scaler.inverse_transform(test_x[:, -1, variable].reshape(-1,1)) # MM5\n","\n","        predictions_array, targets_array = evaluate(model, test_x, test_y, label_scaler, (variable in POSITIVE_VARIABLES) and POST_PROCESS, mm5_array)\n","\n","\n","        filename_xlsx = root + 'arrays'+suffix+'.xlsx'\n","        formatted_dates = [d.strftime('%d/%m/%Y') for d in test_dates]\n","        df_results = pd.DataFrame({\n","                      'Date': formatted_dates,\n","                      'Hour': test_hours,\n","                      'Real': targets_array.flatten(),\n","                      'MM5': mm5_array.flatten(),\n","                      'Prediction': predictions_array.flatten()\n","                      })\n","\n","        df_results.to_excel(filename_xlsx, index=False)\n","\n","        print('_'*80)\n","\n","        # RMSE ===================================================================\n","        if PRINT_RMSE:\n","          prediction_rmse = rmse(predictions_array, targets_array)\n","          mm5_rmse = rmse(mm5_array, targets_array)\n","          delta_rmse = round((prediction_rmse-mm5_rmse)/mm5_rmse*100,2)\n","\n","          print('Prediction rmse: {:.3f}'.format(prediction_rmse))\n","          print('MM5 rmse: {:.3f}'.format(mm5_rmse))\n","          print('Delta rmse: {:.3f} %'.format(delta_rmse))\n","\n","          report_file.write('Prediction rmse: {:.3f}\\n'.format(prediction_rmse))\n","          report_file.write('MM5 rmse: {:.3f}\\n'.format(mm5_rmse))\n","          report_file.write('Delta rmse: {:.3f} %\\n\\n'.format(delta_rmse))\n","\n","          print('_'*80)\n","\n","        # MAE ===================================================================\n","        prediction_mae = mae(predictions_array, targets_array)\n","        mm5_mae = mae(mm5_array, targets_array)\n","        delta_mae = round((prediction_mae-mm5_mae)/mm5_mae*100,2)*np.sign(mm5_mae)\n","\n","        print('MM5 mae: {:.3f}'.format(mm5_mae))\n","        print('Prediction mae: {:.3f}'.format(prediction_mae))\n","        print('Delta mae: {:.3f} %'.format(delta_mae))\n","\n","        report_file.write('MM5 mae: {:.3f}\\n'.format(mm5_mae))\n","        report_file.write('Prediction mae: {:.3f}\\n'.format(prediction_mae))\n","        report_file.write('Delta mae: {:.3f} %\\n\\n'.format(delta_mae))\n","\n","        print('_'*80)\n","\n","        number_of_outliers = int(len(targets_array)*OUTLIERS)\n","        print('number_of_outliers:',number_of_outliers)\n","\n","        # MAE Outliers ===========================================================\n","        prediction_ae_array=ae_array(predictions_array, targets_array).T[0]\n","        mm5_ae_array=ae_array(mm5_array, targets_array).T[0]\n","\n","        indices_of_prediction_outliers = np.argpartition(prediction_ae_array, -number_of_outliers)[-number_of_outliers:]\n","        indices_of_mm5_outliers = np.argpartition(mm5_ae_array, -number_of_outliers)[-number_of_outliers:]\n","\n","        indices_of_outliers = list(set(indices_of_prediction_outliers) | set(indices_of_mm5_outliers))\n","\n","        print(len(predictions_array))\n","        print(len(indices_of_outliers))\n","\n","        outliers_prediction_mae = mae(predictions_array[indices_of_outliers], targets_array[indices_of_outliers])\n","        outliers_mm5_mae = mae(mm5_array[indices_of_outliers], targets_array[indices_of_outliers])\n","        outliers_delta_mae = round((outliers_prediction_mae-outliers_mm5_mae)/outliers_mm5_mae*100,2)*np.sign(outliers_mm5_mae)\n","\n","        print('Outliers MM5 mae: {:.3f}'.format(outliers_mm5_mae))\n","        print('Outliers Prediction mae: {:.3f}'.format(outliers_prediction_mae))\n","        print('Outliers Delta mae: {:.3f} %'.format(outliers_delta_mae))\n","\n","        report_file.write('Outliers MM5 mae: {:.5f}\\n'.format(outliers_mm5_mae))\n","        report_file.write('Outliers Prediction mae: {:.3f}\\n'.format(outliers_prediction_mae))\n","        report_file.write('Outliers Delta mae: {:.3f} %\\n\\n'.format(outliers_delta_mae))\n","\n","        print('_'*80)\n","\n","\n","        # R^2 ====================================================================\n","        r2_prediction = r2_score(targets_array, predictions_array)\n","        r2_mm5 = r2_score(targets_array, mm5_array)\n","        delta_r2 = round((r2_prediction-r2_mm5)/r2_mm5*100,2)*np.sign(r2_mm5)\n","\n","        print('MM5 R²: {:.3f}'.format(r2_mm5))\n","        print('Prediction R²: {:.3f}'.format(r2_prediction))\n","        print('Delta R²: {:.3f} %'.format(delta_r2))\n","\n","        report_file.write('MM5 R²: {:.3f}\\n'.format(r2_mm5))\n","        report_file.write('Prediction R²: {:.3f}\\n'.format(r2_prediction))\n","        report_file.write('Delta R²: {:.3f} %\\n\\n'.format(delta_r2))\n","\n","        print('_'*80)\n","\n","        # R^2 Outliers ===========================================================\n","        prediction_se_array=se_array(predictions_array, targets_array).T[0]\n","        mm5_se_array=se_array(mm5_array, targets_array).T[0]\n","\n","        indices_of_prediction_outliers = np.argpartition(prediction_se_array, -number_of_outliers)[-number_of_outliers:]\n","        indices_of_mm5_outliers = np.argpartition(mm5_se_array, -number_of_outliers)[-number_of_outliers:]\n","\n","        indices_of_outliers = list(set(indices_of_prediction_outliers) | set(indices_of_mm5_outliers))\n","\n","        print(len(predictions_array))\n","        print(len(indices_of_outliers))\n","\n","        outliers_r2_prediction = r2_score(predictions_array[indices_of_outliers], targets_array[indices_of_outliers])\n","        outliers_r2_mm5 = r2_score(mm5_array[indices_of_outliers], targets_array[indices_of_outliers])\n","        outliers_delta_r2 = round((outliers_r2_prediction-outliers_r2_mm5)/outliers_r2_mm5*100,2)*np.sign(outliers_r2_mm5)\n","\n","        print('Outliers MM5 R²: {:.3f}'.format(outliers_r2_mm5))\n","        print('Outliers Prediction R²: {:.3f}'.format(outliers_r2_prediction))\n","        print('Outliers Delta R²: {:.3f} %'.format(outliers_delta_r2))\n","\n","        report_file.write('Outliers MM5 R²: {:.3f}\\n'.format(outliers_r2_mm5))\n","        report_file.write('Outliers Prediction R²: {:.3f}\\n'.format(outliers_r2_prediction))\n","        report_file.write('Outliers Delta R²: {:.3f} %\\n\\n'.format(outliers_delta_r2))\n","\n","        print('_'*80)\n","\n","        # LateX\n","        tabular='\\\\begin{tabular}[c]{@{}c@{}} '+\\\n","        latex_row(mm5_mae,prediction_mae,delta_mae, False) +\\\n","        '\\end{tabular} & \\\\begin{tabular}[c]{@{}c@{}}'+\\\n","        latex_row(r2_mm5,r2_prediction,delta_r2, True) +\\\n","        '\\end{tabular}'\n","\n","        outliers_tabular='\\\\begin{tabular}[c]{@{}c@{}} '+\\\n","        latex_row(outliers_mm5_mae,outliers_prediction_mae,outliers_delta_mae, False) +\\\n","        '\\end{tabular} & \\\\begin{tabular}[c]{@{}c@{}}'+\\\n","        latex_row(outliers_r2_mm5,outliers_r2_prediction,outliers_delta_r2, True) +\\\n","        '\\end{tabular}'\n","\n","        report_file.write(tabular+'\\n\\n')\n","        report_file.write(outliers_tabular)\n","\n","        report_file.close()\n","\n","        # Plot ===================================================================\n","        days=(TEST_PLOT_STOP-TEST_PLOT_START)//144\n","        print('Plotting results over ', days,' days',sep='')\n","\n","        shift=520#(3-LOOK_FORWARD_DAYS)*260\n","\n","        plt.figure(figsize=(20,10))\n","        plt.subplot(2,2,1)\n","        plt.plot(targets_array[TEST_PLOT_START+shift:TEST_PLOT_STOP+shift], color=\"g\", label=\"Real\", linewidth=0.7)\n","        plt.plot(mm5_array[TEST_PLOT_START+shift:TEST_PLOT_STOP+shift], color=\"b\", label=\"MM5\", linestyle=\"dashed\", linewidth=0.7)\n","        plt.plot(predictions_array[TEST_PLOT_START+shift:TEST_PLOT_STOP+shift], color=\"r\", label=\"Prediction\", linestyle=\"dashdot\", linewidth=0.7)\n","        plt.ylabel(VARIABLE_NAMES[variable]+' ['+UNITS_OF_MEASUREMENT[variable]+']')\n","        plt.xlabel('Time')\n","        plt.legend()\n","\n","        # Save the figure ========================================================\n","        figure_name = root + 'figure'+suffix+'.png'\n","        plt.savefig(figure_name, dpi=(250), bbox_inches='tight')\n","        plt.show()\n","        if PLOT_DISTRIBUTIONS:\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQVbArUV29-t"},"outputs":[],"source":["if not PLOT_DISTRIBUTIONS:\n","    # Plot =====================================================================\n","    TEST_PLOT_START = 5100#6710\n","    TEST_PLOT_STOP = TEST_PLOT_START + 144*3\n","\n","    days=(TEST_PLOT_STOP-TEST_PLOT_START)//144\n","    print('Plotting results over ', days,' days',sep='')\n","\n","    plt.figure(figsize=(20,10))\n","    plt.subplot(2,2,1)\n","    plt.plot(targets_array[TEST_PLOT_START:TEST_PLOT_STOP], color=\"g\", label=\"Real\", linewidth=0.7)\n","    plt.plot(mm5_array[TEST_PLOT_START:TEST_PLOT_STOP], color=\"b\", label=\"MM5\", linestyle='dashed', linewidth=0.7)\n","    plt.plot(predictions_array[TEST_PLOT_START:TEST_PLOT_STOP], color=\"r\", label=\"Predicted\", linestyle='dashdot', linewidth=0.7)\n","    plt.ylabel(VARIABLE_NAMES[variable])\n","    plt.legend()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"18nRBfkmCCcYfPM-PwzPP8Gk9yBLHWBuY","timestamp":1742029922741},{"file_id":"1igAoJ5pMKzvIP1en66ZHGl884qSOwU0x","timestamp":1738935041133},{"file_id":"1e4HkaDdr0pnxII7jAPZlWXUKGxqHa5yw","timestamp":1718975870218},{"file_id":"1z_4jlkIjNYLa4ETgBxDOSrpH58dSsUpM","timestamp":1696155942203}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":0}
