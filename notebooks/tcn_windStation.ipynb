{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x-Xm_Qh93JdR"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from pathlib import Path\n",
        "import random\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Z24XQ0BEgY"
      },
      "outputs": [],
      "source": [
        "# TCN\n",
        "!pip install pytorch-tcn\n",
        "from pytorch_tcn import TCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM3R1mZkowpk"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import os\n",
        "\n",
        "# google_drive_path = \"G:/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JVjLjygN_UFK"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtofuMh-kCkc"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "    print('and then re-execute this cell.')\n",
        "else:\n",
        "    print(gpu_info)\n",
        "\n",
        "CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "\n",
        "# torch and python versions ====================================================\n",
        "print('torch.__version__:',torch.__version__)\n",
        "print('sys.version:',sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Aobf1S6lTIne"
      },
      "outputs": [],
      "source": [
        "DATASET_NAMES = ['Site 3']\n",
        "VARIABLE_NAMES = ['Temperature', 'V', 'Sin(dir)','Cos(dir)']\n",
        "UNITS_OF_MEASUREMENT = ['°C', 'm/s', '', '']\n",
        "POSITIVE_VARIABLES = [1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters ==============================================================\n",
        "LOOK_FORWARD_DAYS_LIST = [1,2,3]\n",
        "DATASETS_TO_PROCESS = [0]\n",
        "VARIABLES_TO_PROCESS = [1,2,3]\n",
        "\n",
        "LOOK_BACK_DAYS = 3\n",
        "TEST_FRACTION = 0.2\n",
        "VALIDATION_FRACTION = 0.2\n",
        "PATIENCE = 10\n",
        "SHUFFLE = True\n",
        "FORCE_TRAINING = False\n",
        "DEBUG = False\n",
        "USE_ADAM = False\n",
        "USE_EARLY_STOP = True\n",
        "USE_VALIDATION = True\n",
        "DAYS_TO_PLOT = 3\n",
        "POST_PROCESS = True\n",
        "PRINT_RMSE = False\n",
        "\n",
        "STANDARD_SCALERS = False\n",
        "PLOT_DISTRIBUTIONS = False\n",
        "OUTLIERS = 0.25\n",
        "# ==============================================================================\n",
        "\n",
        "if not USE_VALIDATION:\n",
        "  VALIDATION_FRACTION = 0.0\n",
        "\n",
        "MODELS = ['gru', 'tcn']\n",
        "\n",
        "HP = {0: # Site 3\n",
        "         {0:  { 'HIDDEN_DIM' : 128,\n",
        "                'N_LAYERS' : 2,\n",
        "                'BATCH_SIZE' : 512,\n",
        "                'LEARNING_RATE' : 0.001,\n",
        "                'TEST_PLOT_START' : [9035, 8920, 8800],\n",
        "                'EPOCHS' : 200,\n",
        "                'NUM_CHANNELS' : [32, 64],\n",
        "                'KERNEL_SIZE' : 3,\n",
        "                'SKIP_CONNECTIONS' : False},\n",
        "\n",
        "          1:  { 'HIDDEN_DIM' :128,\n",
        "                'N_LAYERS' : 2,\n",
        "                'BATCH_SIZE' : 512,\n",
        "                'LEARNING_RATE' : 0.001,\n",
        "                'TEST_PLOT_START' : [9035, 8920, 8800],\n",
        "                'EPOCHS' : 200,\n",
        "                'NUM_CHANNELS' : [32, 64],\n",
        "                'KERNEL_SIZE' : 3,\n",
        "                'SKIP_CONNECTIONS' : False},\n",
        "\n",
        "          2:  { 'HIDDEN_DIM' : 128,\n",
        "                'N_LAYERS' : 2, #4\n",
        "                'BATCH_SIZE' : 512, #256\n",
        "                'LEARNING_RATE' : 0.001,\n",
        "                'TEST_PLOT_START' : [9035, 8920, 8800],\n",
        "                'EPOCHS' : 200,\n",
        "                'NUM_CHANNELS' : [32, 64],\n",
        "                'KERNEL_SIZE' : 3,\n",
        "                'SKIP_CONNECTIONS' : False},\n",
        "\n",
        "          3:  { 'HIDDEN_DIM' : 128,\n",
        "                'N_LAYERS' : 2, #4\n",
        "                'BATCH_SIZE' : 512, #256\n",
        "                'LEARNING_RATE' : 0.001,  #0.0001\n",
        "                'TEST_PLOT_START' : [9035, 8920, 8800],\n",
        "                'EPOCHS' : 200,\n",
        "                'NUM_CHANNELS' : [32, 64],\n",
        "                'KERNEL_SIZE' : 3,\n",
        "                'SKIP_CONNECTIONS' : False}\n",
        "        },\n",
        "}"
      ],
      "metadata": {
        "id": "0ouotLUMyAUN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L8cY7Aim3Ak5"
      },
      "outputs": [],
      "source": [
        "# Loop through each dataset in DATASETS_TO_PROCESS\n",
        "for dataset_id in DATASETS_TO_PROCESS:\n",
        "  # Access the configuration dictionary for the current dataset\n",
        "  dataset_config = HP[dataset_id]\n",
        "\n",
        "  # Loop through each variable configuration within the dataset\n",
        "  for variable_id in VARIABLES_TO_PROCESS:\n",
        "    # Access the 'HIDDEN_DIM' value for the current variable\n",
        "    hidden_dim = dataset_config[variable_id]['HIDDEN_DIM']\n",
        "  for variable_id in VARIABLES_TO_PROCESS:\n",
        "    # Access the 'N_LAYERS' value for the current variable\n",
        "    n_layers = dataset_config[variable_id]['N_LAYERS']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_OfuAA_A_mXM"
      },
      "outputs": [],
      "source": [
        "def process_epoch(model, data_loader, criterion, optimizer, train):\n",
        "    n_layers = model.n_layers\n",
        "    hidden_dim = model.hidden_dim\n",
        "    batch_size = data_loader.batch_size\n",
        "\n",
        "    if type(model) == GRUModel:\n",
        "      if SHUFFLE:\n",
        "        h = torch.zeros([n_layers, batch_size, hidden_dim]).to(device)\n",
        "      else:\n",
        "        h = model.init_hidden(batch_size)\n",
        "\n",
        "    total_loss = 0.\n",
        "    counter = 0\n",
        "\n",
        "    for x, y, local_means in data_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if type(model) == GRUModel:\n",
        "          if SHUFFLE:\n",
        "            h = torch.zeros([n_layers, batch_size, hidden_dim]).to(device)\n",
        "          else:\n",
        "            h = h.data\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        if type(model) == GRUModel:\n",
        "          y_hat, h = model(x.to(device).float(), h)\n",
        "          # print('Shape H:',h.shape)\n",
        "        elif type(model) == TCNModel:\n",
        "          y_hat = model(x.to(device).float())\n",
        "\n",
        "        base_loss = criterion(y_hat, y.to(device).float())\n",
        "\n",
        "        #local_mean_loss = torch.mean(torch.abs(y_hat - local_means.to(device).float()))  #l1\n",
        "        #local_mean_loss = torch.mean((y_hat - local_means.to(device).float()) ** 2) # mse\n",
        "        local_mean_loss = criterion(y_hat, local_means.to(device).float())\n",
        "\n",
        "        α = 0.5\n",
        "        #loss = base_loss\n",
        "        #loss = local_mean_loss\n",
        "        loss = α * base_loss + (1-α) * local_mean_loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if counter%20==0:\n",
        "            print(\"Step: {}/{}, Average Loss: {}\".format(counter, len(data_loader), total_loss/counter))\n",
        "\n",
        "    avg_loss = total_loss/len(data_loader)\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "def train(model_name, train_loader, validation_loader, learning_rate, positive_variable, batch_size, hidden_dim=64, num_channels = None, kernel_size = None, skip_connections = False, n_layers=2, patience=None, epochs=None):\n",
        "    input_dim = next(iter(train_loader))[0].shape[2]\n",
        "    output_dim = 1\n",
        "\n",
        "    print('Input dim.:', input_dim)\n",
        "    print('Output dim.:', output_dim)\n",
        "\n",
        "    if 'tcn' in model_name:\n",
        "      print(\"tcn è in model_name\")\n",
        "      model = TCNModel(input_dim, hidden_dim, output_dim, n_layers, positive_variable, kernel_size, num_channels, skip_connections)\n",
        "    else:\n",
        "      model = GRUModel(input_dim, hidden_dim, output_dim, n_layers, positive_variable)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    #criterion = nn.MSELoss()\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    if USE_ADAM:\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "      optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch=1\n",
        "    patience_counter = 0\n",
        "    min_validation_loss=None\n",
        "    validation_loss=None\n",
        "\n",
        "    train_loss_list=[]\n",
        "    validation_loss_list=[]\n",
        "\n",
        "    # Training loop ============================================================\n",
        "    while True:\n",
        "        print(\"Epoch {}\".format(epoch))\n",
        "\n",
        "        print('Training...')\n",
        "        train_loss = process_epoch(model, train_loader, criterion, optimizer, train = True)\n",
        "\n",
        "        if USE_VALIDATION:\n",
        "          print('Validation...')\n",
        "          validation_loss = process_epoch(model, validation_loader, criterion, optimizer, train = False)\n",
        "          validation_loss_list.append(validation_loss)\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "\n",
        "        if min_validation_loss is None or validation_loss<min_validation_loss or not USE_VALIDATION:\n",
        "          # Save the model\n",
        "          print('Saving the model...')\n",
        "          if 'gru' in model_name:\n",
        "            torch.save(model, model_name)\n",
        "          elif 'tcn' in model_name:\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "\n",
        "          min_validation_loss = validation_loss\n",
        "          patience_counter = 0\n",
        "        else:\n",
        "          print('Increasing patience counter')\n",
        "          patience_counter +=1\n",
        "\n",
        "        print(\"Epoch {} Completed, Train Loss: {}, Validation Loss: {}, Patience counter: {}\".format(epoch, train_loss, validation_loss, patience_counter))\n",
        "\n",
        "        if (patience_counter>patience and USE_EARLY_STOP) or epoch==epochs:\n",
        "          break\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    print('Restoring the best model...')\n",
        "    if 'gru' in model_name:\n",
        "      model = torch.load(model_name, weights_only=False)\n",
        "    elif 'tcn' in model_name:\n",
        "      model.load_state_dict(torch.load(model_name))\n",
        "    return model, train_loss_list, validation_loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s0xmBG49c_Kr"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Model(nn.Module, ABC):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, positive_variable, num_channels = None, drop_prob=0.0): # num_channels = None because it\n",
        "        super().__init__()                                                                                                  # doesn't exist in the rnns\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.positive_variable=positive_variable\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x, h = None): # h = None because it doesn't exist in the tcns\n",
        "        pass\n",
        "\n",
        "class GRUModel(Model):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, positive_variable, drop_prob=0.0):\n",
        "        super().__init__(input_dim, hidden_dim, output_dim, n_layers, positive_variable, drop_prob)\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "\n",
        "        #self.bat_a = nn.BatchNorm1d(hidden_dim)\n",
        "        #self.bat_b = nn.BatchNorm1d(hidden_dim)\n",
        "        self.activation_a = nn.ReLU()\n",
        "        #self.activation_b = nn.ReLU()\n",
        "        self.fc_a = nn.Linear(hidden_dim, hidden_dim//2)\n",
        "        self.fc_b = nn.Linear(hidden_dim//2, output_dim)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "\n",
        "        # The following statement is important: we are implementing a S2V model,\n",
        "        # so we select the latest output.\n",
        "\n",
        "        out=out[:,-1]\n",
        "\n",
        "        #out=self.activation_a(out)\n",
        "        out=self.fc_a(out)\n",
        "        out=self.activation_a(out)\n",
        "        out=self.fc_b(out)\n",
        "        if self.positive_variable:\n",
        "          #out=torch.exp(out)\n",
        "          #out=torch.abs(out)\n",
        "          #out=torch.square(out)\n",
        "          pass\n",
        "\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        print(\"self.parameters() = \", self.parameters())\n",
        "        print(\"next(self.parameters()) = \", next(self.parameters()))\n",
        "        print(\"weight = next(self.parameters()).data = \", weight)\n",
        "        print(\"type of weight = \", type(weight))\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        print(\"hidden=weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\", weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        return hidden\n",
        "\n",
        "class TCNModel(Model):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, positive_variable, kernel_size, num_channels=None, skip_connections = False, drop_prob=0.0):\n",
        "        super().__init__(input_dim, hidden_dim, output_dim, n_layers, positive_variable, num_channels=None, drop_prob=0.0)\n",
        "        if num_channels is None:  # if channels is not specified use uniform hidden_size\n",
        "          num_channels = [hidden_dim] * n_layers\n",
        "        else:\n",
        "          self.num_channels = num_channels\n",
        "\n",
        "        self.tcn = TCN(input_dim, num_channels, kernel_size, dropout=drop_prob, causal=True, use_skip_connections=skip_connections)\n",
        "        self.fc = nn.Linear(num_channels[-1], output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #TCN expects input shape (batch_size, input size, sequence length)\n",
        "        out = self.tcn(x.transpose(1, 2))\n",
        "        out = out[:, :, -1] # it takes the output of the last timestep\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UtcsBXq9PYJw"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_x, test_y, label_scaler, post_process, mm5_array):\n",
        "    outputs = []\n",
        "    targets = []\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        inp = torch.from_numpy(np.expand_dims(test_x[i], axis=0))\n",
        "\n",
        "        if type(model) == GRUModel:\n",
        "          out, h = model(inp.to(device).float(), None)\n",
        "        elif type(model) == TCNModel:\n",
        "          out = model(inp.to(device).float())\n",
        "\n",
        "        outputs.append(out[0,0].item())\n",
        "        targets.append(test_y[i])\n",
        "\n",
        "        if i%5000==0:\n",
        "          print(i,'/',test_x.shape[0],sep='')\n",
        "\n",
        "    predictions_array = label_scaler.inverse_transform(np.array(outputs).reshape(-1, 1))\n",
        "    target_array = label_scaler.inverse_transform(np.array(targets).reshape(-1, 1))\n",
        "\n",
        "    if post_process:\n",
        "      predictions_array = predictions_array * (mm5_array!=0)\n",
        "\n",
        "    return predictions_array, target_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmI28SVo89z9"
      },
      "outputs": [],
      "source": [
        "def se_array(x, y):\n",
        "  return np.square(x-y)\n",
        "\n",
        "def ae_array(x, y):\n",
        "  return np.abs(x - y)\n",
        "\n",
        "def rmse(x,y):\n",
        "  return np.sqrt(mean_squared_error(x,y))\n",
        "\n",
        "def mae(x, y):\n",
        "    return np.mean(np.abs(x-y))\n",
        "\n",
        "def latex_row(a,b,c,bigger):\n",
        "  if a>b and bigger or a<b and not bigger:\n",
        "    return '\\\\textbf{'+str(round(a, 3))+'}\\\\\\\\ '+str(round(b, 3))+'\\\\\\\\ '+str(round(c, 2))+'\\%'\n",
        "  else:\n",
        "    return str(round(a, 3))+'\\\\\\\\ \\\\textbf{'+str(round(b, 3))+'}\\\\\\\\ '+str(round(c, 2))+'\\%'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sBfUNdvC8NSM"
      },
      "outputs": [],
      "source": [
        "def calculate_local_mean(values, window_size=3):\n",
        "    result = np.zeros_like(values)\n",
        "    padded = np.pad(values, (window_size, window_size), mode='edge')\n",
        "\n",
        "    for i in range(len(values)):\n",
        "        window_start = i\n",
        "        window_end = i + 2 * window_size + 1\n",
        "        result[i] = np.mean(padded[window_start:window_end])\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IYM0wA0NQRcV"
      },
      "outputs": [],
      "source": [
        "#site3_mm5_mae_1 = []\n",
        "#site3_gru_mae_1 = []\n",
        "site3_tcn_mae_1 = []\n",
        "\n",
        "#site3_mm5_mae_2 = []\n",
        "#site3_gru_mae_2 = []\n",
        "site3_tcn_mae_2 = []\n",
        "\n",
        "\n",
        "#site3_mm5_mae_3 = []\n",
        "#site3_gru_mae_3 = []\n",
        "site3_tcn_mae_3 = []\n",
        "\n",
        "\n",
        "#site3_mm5_r2_1 = []\n",
        "#site3_gru_r2_1 = []\n",
        "site3_tcn_r2_1 = []\n",
        "\n",
        "\n",
        "#site3_mm5_r2_2 = []\n",
        "#site3_gru_r2_2 = []\n",
        "site3_tcn_r2_2 = []\n",
        "\n",
        "#site3_mm5_r2_3 = []\n",
        "#site3_gru_r2_3 = []\n",
        "site3_tcn_r2_3 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmR7Sty1Vla0"
      },
      "outputs": [],
      "source": [
        "for LOOK_FORWARD_DAYS in [3]:\n",
        "  for dataset in DATASETS_TO_PROCESS:\n",
        "    for variable in VARIABLES_TO_PROCESS:\n",
        "      print('='*80)\n",
        "      print('Look Forward Days:', LOOK_FORWARD_DAYS)\n",
        "      print('Dataset:', DATASET_NAMES[dataset])\n",
        "      print('Variable:', VARIABLE_NAMES[variable])\n",
        "      print('_'*80)\n",
        "\n",
        "      if variable not in HP[dataset].keys():\n",
        "        continue\n",
        "\n",
        "      suffix = '_look_forward_days_' + str(LOOK_FORWARD_DAYS) + '_dataset_' + str(dataset) + '_variable_' + str(variable)\n",
        "\n",
        "      data_root = '/content/drive/MyDrive/Colab Notebooks/datasets/energy/'\n",
        "      root_tcn = data_root + 'results/'\n",
        "\n",
        "      report_name_tcn = root_tcn + 'report'+suffix+'.txt'\n",
        "      report_file_tcn = open(report_name_tcn, \"w\")\n",
        "\n",
        "      report_file_tcn.write('Dataset {}, Variable {}\\n'.format(str(dataset), VARIABLE_NAMES[variable]))\n",
        "      report_file_tcn.write('_'*80+'\\n')\n",
        "\n",
        "      # Create dataframe =======================================================\n",
        "\n",
        "      # ============================================================================\n",
        "      # DATA LOADING\n",
        "      # ============================================================================\n",
        "      # NOTE: Due to confidentiality agreements, the datasets cannot be publicly\n",
        "      # shared. The code is provided for transparency and reproducibility of the\n",
        "      # methodology. Researchers with similar datasets can adapt this code for\n",
        "      # their own data.\n",
        "      #\n",
        "      # Expected file structure in Google Drive:\n",
        "      # MyDrive/Colab Notebooks/datasets/energy/dataset_{0,1}_look_forward_days_{1,2,3}.xlsx\n",
        "      # ============================================================================\n",
        "\n",
        "      url = data_root + 'dataset_'+str(dataset)+'_look_forward_days_' + str(LOOK_FORWARD_DAYS)+'.xlsx'\n",
        "      df = pd.read_excel(url)\n",
        "\n",
        "      pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "      #print(df.head)\n",
        "\n",
        "      df['dayofyear'] = df.apply(lambda x: x['Data_R'].dayofyear,axis=1)\n",
        "      df['hour'] = df.apply(lambda x: x['Ora_R'].hour,axis=1)\n",
        "      #df['dayofweek'] = df.apply(lambda x: x['Data_R'].dayofweek,axis=1)\n",
        "      #df['month'] = df.apply(lambda x: x['Data_R'].month,axis=1)\n",
        "      #df['minute'] = df.apply(lambda x: x['Ora_R'].minute,axis=1)\n",
        "\n",
        "      datetime_info = df[['Data_R', 'Ora_R']].copy()\n",
        "\n",
        "      df = df.drop([\"Data_R\", \"Ora_R\", \"Data_MM5\", \"Ora_MM5\"], axis=1)\n",
        "      df = df.replace(\"None\", np.nan)\n",
        "\n",
        "      if PLOT_DISTRIBUTIONS:\n",
        "        df[df['GHI_R'].notna()]['GHI_R'].plot.hist(bins=50) # notna() This function takes a scalar or array-like object and indicates whether values\n",
        "        plt.pause(0.001)                                    # are valid (not missing)\n",
        "        df[df['Temperatura_R'].notna()]['Temperatura_R'].plot.hist(bins=100)\n",
        "        plt.pause(0.001)\n",
        "        df[df['Pressione_R'].notna()]['Pressione_R'].plot.hist(bins=100)\n",
        "        plt.pause(0.001)\n",
        "        df[df['Umidità_R'].notna()]['Umidità_R'].plot.hist(bins=100)\n",
        "        plt.pause(0.001)\n",
        "        break\n",
        "\n",
        "      NUMBER_OF_VARIABLES = (len(df.columns)-2)//2\n",
        "      print('Number of variables:', NUMBER_OF_VARIABLES)\n",
        "\n",
        "      # Select current hyperparameters =========================================\n",
        "      HIDDEN_DIM =HP[dataset][variable]['HIDDEN_DIM']\n",
        "      N_LAYERS = HP[dataset][variable]['N_LAYERS']\n",
        "      BATCH_SIZE = HP[dataset][variable]['BATCH_SIZE']\n",
        "      LEARNING_RATE = HP[dataset][variable]['LEARNING_RATE']\n",
        "      TEST_PLOT_START = HP[dataset][variable]['TEST_PLOT_START'][LOOK_FORWARD_DAYS-1]\n",
        "      TEST_PLOT_STOP = TEST_PLOT_START+144*DAYS_TO_PLOT\n",
        "      EPOCHS = HP[dataset][variable]['EPOCHS']\n",
        "      NUM_CHANNELS = HP[dataset][variable]['NUM_CHANNELS']\n",
        "      KERNEL_SIZE = HP[dataset][variable]['KERNEL_SIZE']\n",
        "      SKIP_CONNECTIONS = HP[dataset][variable]['SKIP_CONNECTIONS']\n",
        "\n",
        "\n",
        "\n",
        "      for hp in HP[dataset][variable].keys():\n",
        "        print(hp,': ', HP[dataset][variable][hp],sep='')\n",
        "        pass\n",
        "\n",
        "      # Create scalers =========================================================\n",
        "      data=df.astype('float32').to_numpy()\n",
        "      print(df.head())\n",
        "\n",
        "      if STANDARD_SCALERS:\n",
        "        scaler = StandardScaler()\n",
        "        label_scaler = StandardScaler()\n",
        "        mm5_scaler = StandardScaler()\n",
        "      else:\n",
        "        scaler = MinMaxScaler()\n",
        "        label_scaler = MinMaxScaler()\n",
        "        mm5_scaler = MinMaxScaler()\n",
        "\n",
        "      if dataset==0 or variable==0:\n",
        "        label_scaler.fit(data[:,variable].reshape(-1,1))\n",
        "        mm5_scaler.fit(data[:,variable+NUMBER_OF_VARIABLES].reshape(-1,1))\n",
        "      else:\n",
        "        label_scaler.fit(data[:,variable-1].reshape(-1,1))\n",
        "        mm5_scaler.fit(data[:,variable+NUMBER_OF_VARIABLES-1].reshape(-1,1))\n",
        "\n",
        "      data = scaler.fit_transform(data)\n",
        "\n",
        "      if DEBUG:\n",
        "        print(scaler.data_min_)\n",
        "        print(scaler.data_max_)\n",
        "        print(label_scaler.data_min_)\n",
        "        print(label_scaler.data_max_)\n",
        "        print(mm5_scaler.data_min_)\n",
        "        print(mm5_scaler.data_max_)\n",
        "\n",
        "      # Define lookback period and split inputs/labels =========================\n",
        "      look_back_steps = 144 * LOOK_BACK_DAYS\n",
        "      look_forward_steps = 144 * LOOK_FORWARD_DAYS\n",
        "\n",
        "      #inputs = np.zeros((len(data)-look_back_steps-look_forward_steps+1,look_back_steps,df.shape[1]))\n",
        "      inputs = np.zeros((len(data)-look_back_steps-look_forward_steps+1,look_back_steps,NUMBER_OF_VARIABLES+2)) # MM5\n",
        "\n",
        "      local_means = np.zeros(len(data)-look_back_steps-look_forward_steps+1)\n",
        "\n",
        "      labels = np.zeros(len(data)-look_back_steps-look_forward_steps+1)\n",
        "      dates = np.zeros(len(data)-look_back_steps-look_forward_steps+1, dtype='object')\n",
        "      hours = np.zeros(len(data)-look_back_steps-look_forward_steps+1, dtype='object')\n",
        "\n",
        "      if DEBUG:\n",
        "        print(data.shape)\n",
        "        print(inputs.shape)\n",
        "        print(labels.shape)\n",
        "\n",
        "      mask=np.full(inputs.shape[0], True)\n",
        "\n",
        "      # ONLY MM5 DATA:\n",
        "\n",
        "      # The input is a sequence of vectors containing the MM5 values from time\n",
        "      # 'T-look_back_steps+1' to time 'T' (where T is future instant of time the\n",
        "      # prediction is referred to)\n",
        "      # Try look_back_steps=look_forward_steps=1\n",
        "\n",
        "      # The models takes in input only MM5 values that cannot be null\n",
        "      # That's why some data rows previously deleted now are included and the\n",
        "      # MM5 performance metrics are slightly different.\n",
        "\n",
        "      for i in range(look_back_steps, len(data) - look_forward_steps + 1):\n",
        "\n",
        "        index = i-look_back_steps\n",
        "\n",
        "        #inputs[index] = data[index : i]\n",
        "        inputs[index] = data[index : i, NUMBER_OF_VARIABLES:] # MM5\n",
        "\n",
        "        if dataset==0 or variable==0:\n",
        "          labels[index] = data[i + look_forward_steps - 1, variable]\n",
        "        else:\n",
        "          labels[index] = data[i + look_forward_steps - 1, variable-1]\n",
        "\n",
        "        dates[index] = datetime_info['Data_R'][i + look_forward_steps - 1]\n",
        "        hours[index] = datetime_info['Ora_R'][i + look_forward_steps - 1]\n",
        "\n",
        "        if np.isnan(inputs[index]).any() or np.isnan(labels[index]).any():\n",
        "          #print('input:',inputs[index])\n",
        "          #print('label:',labels[index])\n",
        "          mask[index] = False\n",
        "\n",
        "        if i%10000==0 and DEBUG:\n",
        "          print(i)\n",
        "          print('-'*80)\n",
        "          print(inputs[index])\n",
        "          print(labels[index])\n",
        "\n",
        "      if DEBUG or True:\n",
        "        print('BEFORE')\n",
        "        print(inputs.shape)\n",
        "        print(labels.shape)\n",
        "        print('nan inputs:', np.isnan(inputs).any())\n",
        "        print('nan labels:', np.isnan(labels).any())\n",
        "        print('Number of nan tuples:', np.sum(np.logical_not(mask)))\n",
        "\n",
        "      inputs=inputs[mask]\n",
        "      labels=labels[mask]\n",
        "      dates = dates[mask]\n",
        "      hours = hours[mask]\n",
        "\n",
        "      if DEBUG or True:\n",
        "        print('AFTER')\n",
        "        print(inputs.shape)\n",
        "        print(labels.shape)\n",
        "        print('nan inputs:', np.isnan(inputs).any())\n",
        "        print('nan labels:', np.isnan(labels).any())\n",
        "\n",
        "      #inputs = inputs.reshape(-1, look_back_steps, df.shape[1])\n",
        "      inputs = inputs.reshape(-1, look_back_steps, NUMBER_OF_VARIABLES +2) # MM5\n",
        "      labels = labels.reshape(-1, 1)\n",
        "\n",
        "      local_means = calculate_local_mean(labels.flatten())\n",
        "      local_means = local_means.reshape(-1, 1)\n",
        "\n",
        "      # Split data into training set, validation set and test set\n",
        "      validation_portion = int(VALIDATION_FRACTION*len(inputs))\n",
        "      test_portion = int(TEST_FRACTION*len(inputs))\n",
        "      validation_test_portion = validation_portion + test_portion\n",
        "\n",
        "      train_x = inputs[:-validation_test_portion]\n",
        "      train_y = labels[:-validation_test_portion]\n",
        "      train_local_means = local_means[:-validation_test_portion]\n",
        "\n",
        "      validation_x = inputs[-validation_test_portion:-test_portion]\n",
        "      validation_y = labels[-validation_test_portion:-test_portion]\n",
        "      validation_local_means = local_means[-validation_test_portion:-test_portion]\n",
        "\n",
        "      test_x = inputs[-test_portion:]\n",
        "      test_y = labels[-test_portion:]\n",
        "      test_dates = dates[-test_portion:]\n",
        "      test_hours = hours[-test_portion:]\n",
        "\n",
        "      # Create dataloaders =====================================================\n",
        "      train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y), torch.from_numpy(train_local_means))\n",
        "      train_loader = DataLoader(train_data, shuffle=SHUFFLE, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "      validation_data = TensorDataset(torch.from_numpy(validation_x), torch.from_numpy(validation_y), torch.from_numpy(validation_local_means))\n",
        "      validation_loader = DataLoader(validation_data, shuffle=SHUFFLE, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "      train_loss_list_gru=None\n",
        "      train_loss_list_tcn=None\n",
        "      validation_loss_list_gru=None\n",
        "      validation_loss_list_tcn=None\n",
        "\n",
        "      # Create the model =======================================================\n",
        "      model_name_tcn = root_tcn + 'model' + suffix + '.zip'\n",
        "\n",
        "      if Path(model_name_tcn).is_file() and not FORCE_TRAINING:\n",
        "          input_dim = next(iter(train_loader))[0].shape[2]\n",
        "          model_tcn = TCNModel(input_dim, hidden_dim=HIDDEN_DIM, output_dim=1, n_layers=N_LAYERS, positive_variable=(variable in POSITIVE_VARIABLES), kernel_size=KERNEL_SIZE, num_channels=NUM_CHANNELS, skip_connections=SKIP_CONNECTIONS)\n",
        "          model_tcn.load_state_dict(torch.load(model_name_tcn))\n",
        "          model_tcn.to(device)\n",
        "      else:\n",
        "          print(\"training start\")\n",
        "          model_tcn, train_loss_list_tcn, validation_loss_list_tcn = train(model_name_tcn, train_loader, validation_loader, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE, hidden_dim=HIDDEN_DIM, n_layers=N_LAYERS, positive_variable=(variable in POSITIVE_VARIABLES), patience=PATIENCE, epochs=EPOCHS, num_channels=NUM_CHANNELS, kernel_size=KERNEL_SIZE, skip_connections=SKIP_CONNECTIONS)\n",
        "\n",
        "      model_tcn.to(device)\n",
        "      model_tcn.eval()\n",
        "\n",
        "\n",
        "\n",
        "      # Evaluation =============================================================\n",
        "\n",
        "      if dataset==0 or variable==0:\n",
        "        #mm5_array = mm5_scaler.inverse_transform(test_x[:, -1, variable + NUMBER_OF_VARIABLES].reshape(-1,1))\n",
        "        mm5_array = mm5_scaler.inverse_transform(test_x[:, -1, variable].reshape(-1,1)) # MM5\n",
        "      else:\n",
        "        #mm5_array = mm5_scaler.inverse_transform(test_x[:, -1, variable + NUMBER_OF_VARIABLES-1].reshape(-1,1))\n",
        "         mm5_array = mm5_scaler.inverse_transform(test_x[:, -1, variable].reshape(-1,1)) # MM5\n",
        "\n",
        "      predictions_array_tcn, targets_array = evaluate(model_tcn, test_x, test_y, label_scaler, (variable in POSITIVE_VARIABLES) and POST_PROCESS, mm5_array)\n",
        "\n",
        "\n",
        "      filename_xlsx = root_tcn + 'arrays'+suffix+'.xlsx'\n",
        "      formatted_dates = [d.strftime('%d/%m/%Y') for d in test_dates]\n",
        "      df_results = pd.DataFrame({\n",
        "                      'Datae': formatted_dates,\n",
        "                      'Hour': test_hours,\n",
        "                      'Real': targets_array.flatten(),\n",
        "                      'MM5': mm5_array.flatten(),\n",
        "                      'Prediction': predictions_array_tcn.flatten()\n",
        "                    })\n",
        "\n",
        "      df_results.to_excel(filename_xlsx, index=False)\n",
        "\n",
        "      print('_'*80)\n",
        "\n",
        "\n",
        "\n",
        "      # MAE ===================================================================\n",
        "      prediction_mae_tcn = mae(predictions_array_tcn, targets_array)\n",
        "      mm5_mae = mae(mm5_array, targets_array)\n",
        "      delta_mae_tcn = round((prediction_mae_tcn-mm5_mae)/mm5_mae*100,2)*np.sign(mm5_mae)\n",
        "\n",
        "      if dataset == 0:\n",
        "        if LOOK_FORWARD_DAYS == 1:\n",
        "          site3_tcn_mae_1.append(prediction_mae_tcn)\n",
        "        elif LOOK_FORWARD_DAYS == 2:\n",
        "          site3_tcn_mae_2.append(prediction_mae_tcn)\n",
        "        elif LOOK_FORWARD_DAYS == 3:\n",
        "          site3_tcn_mae_3.append(prediction_mae_tcn)\n",
        "\n",
        "      print('MM5 mae: {:.3f}'.format(mm5_mae))\n",
        "      print('Prediction mae tcn: {:.3f}'.format(prediction_mae_tcn))\n",
        "      print('Delta mae tcn: {:.3f} %'.format(delta_mae_tcn))\n",
        "\n",
        "\n",
        "      report_file_tcn.write('MM5 mae: {:.3f}\\n'.format(mm5_mae))\n",
        "      report_file_tcn.write('Prediction mae tcn: {:.3f}'.format(prediction_mae_tcn))\n",
        "      report_file_tcn.write('Delta mae tcn: {:.3f} %'.format(delta_mae_tcn))\n",
        "\n",
        "      print('_'*80)\n",
        "\n",
        "      number_of_outliers = int(len(targets_array)*OUTLIERS)\n",
        "      print('number_of_outliers:',number_of_outliers)\n",
        "\n",
        "\n",
        "      # MAE Outliers ===========================================================\n",
        "      prediction_ae_array_tcn=ae_array(predictions_array_tcn, targets_array).T[0]\n",
        "      mm5_ae_array=ae_array(mm5_array, targets_array).T[0]\n",
        "\n",
        "      indices_of_prediction_outliers_tcn = np.argpartition(prediction_ae_array_tcn, -number_of_outliers)[-number_of_outliers:]\n",
        "      indices_of_mm5_outliers = np.argpartition(mm5_ae_array, -number_of_outliers)[-number_of_outliers:]\n",
        "\n",
        "      indices_of_outliers_tcn = list(set(indices_of_prediction_outliers_tcn) | set(indices_of_mm5_outliers))\n",
        "\n",
        "      print(len(predictions_array_tcn))\n",
        "      print(len(indices_of_outliers_tcn))\n",
        "\n",
        "      outliers_prediction_mae_tcn = mae(predictions_array_tcn[indices_of_outliers_tcn], targets_array[indices_of_outliers_tcn])\n",
        "      outliers_mm5_mae = mae(mm5_array[indices_of_outliers_tcn], targets_array[indices_of_outliers_tcn])\n",
        "      outliers_delta_mae_tcn = round((outliers_prediction_mae_tcn-outliers_mm5_mae)/outliers_mm5_mae*100,2)*np.sign(outliers_mm5_mae)\n",
        "\n",
        "      print('Outliers MM5 mae: {:.3f}'.format(outliers_mm5_mae))\n",
        "      print('Outliers Prediction tcn mae: {:.3f}'.format(outliers_prediction_mae_tcn))\n",
        "      print('Outliers Delta tcn mae: {:.3f} %'.format(outliers_delta_mae_tcn))\n",
        "\n",
        "      report_file_tcn.write('Outliers MM5 mae: {:.5f}\\n'.format(outliers_mm5_mae))\n",
        "      report_file_tcn.write('Outliers Prediction tcn mae: {:.3f}\\n'.format(outliers_prediction_mae_tcn))\n",
        "      report_file_tcn.write('Outliers Delta tcn mae: {:.3f} %\\n\\n'.format(outliers_delta_mae_tcn))\n",
        "      print('_'*80)\n",
        "\n",
        "\n",
        "      # R^2 ====================================================================\n",
        "      r2_prediction_tcn = r2_score(targets_array, predictions_array_tcn)\n",
        "      r2_mm5 = r2_score(targets_array, mm5_array)\n",
        "      delta_r2_tcn = round((r2_prediction_tcn-r2_mm5)/r2_mm5*100,2)*np.sign(r2_mm5)\n",
        "\n",
        "      if dataset == 0:\n",
        "        if LOOK_FORWARD_DAYS == 1:\n",
        "          site3_tcn_r2_1.append(r2_prediction_tcn)\n",
        "        elif LOOK_FORWARD_DAYS == 2:\n",
        "          site3_tcn_r2_2.append(r2_prediction_tcn)\n",
        "        elif LOOK_FORWARD_DAYS == 3:\n",
        "          site3_tcn_r2_3.append(r2_prediction_tcn)\n",
        "\n",
        "      print('MM5 R²: {:.3f}'.format(r2_mm5))\n",
        "      print('Prediction tcn R²: {:.3f}'.format(r2_prediction_tcn))\n",
        "      print('Delta tcn R²: {:.3f} %'.format(delta_r2_tcn))\n",
        "\n",
        "\n",
        "      report_file_tcn.write('MM5 R²: {:.3f}\\n'.format(r2_mm5))\n",
        "      report_file_tcn.write('Prediction tcn R²: {:.3f}'.format(r2_prediction_tcn))\n",
        "      report_file_tcn.write('Delta tcn R²: {:.3f} %'.format(delta_r2_tcn))\n",
        "\n",
        "      print('_'*80)\n",
        "\n",
        "\n",
        "      # R^2 Outliers ===========================================================\n",
        "      prediction_se_array_tcn=se_array(predictions_array_tcn, targets_array).T[0]\n",
        "      mm5_se_array=se_array(mm5_array, targets_array).T[0]\n",
        "\n",
        "      indices_of_prediction_outliers_tcn = np.argpartition(prediction_se_array_tcn, -number_of_outliers)[-number_of_outliers:]\n",
        "      indices_of_mm5_outliers = np.argpartition(mm5_se_array, -number_of_outliers)[-number_of_outliers:]\n",
        "\n",
        "      indices_of_outliers_tcn = list(set(indices_of_prediction_outliers_tcn) | set(indices_of_mm5_outliers))\n",
        "\n",
        "      outliers_r2_prediction_tcn = r2_score(predictions_array_tcn[indices_of_outliers_tcn], targets_array[indices_of_outliers_tcn])\n",
        "      outliers_r2_mm5 = r2_score(mm5_array[indices_of_outliers_tcn], targets_array[indices_of_outliers_tcn])\n",
        "      outliers_delta_r2_tcn = round((outliers_r2_prediction_tcn-outliers_r2_mm5)/outliers_r2_mm5*100,2)*np.sign(outliers_r2_mm5)\n",
        "\n",
        "      print('Outliers MM5 R²: {:.3f}'.format(outliers_r2_mm5))\n",
        "      print('Outliers Prediction tcn R²: {:.3f}'.format(outliers_r2_prediction_tcn))\n",
        "      print('Outliers Delta tcn R²: {:.3f} %'.format(outliers_delta_r2_tcn))\n",
        "\n",
        "      report_file_tcn.write('Outliers MM5 R²: {:.3f}\\n'.format(outliers_r2_mm5))\n",
        "      report_file_tcn.write('Outliers Prediction tcn R²: {:.3f}'.format(outliers_r2_prediction_tcn))\n",
        "      report_file_tcn.write('Outliers Delta tcn R²: {:.3f} %'.format(outliers_delta_r2_tcn))\n",
        "\n",
        "      report_file_tcn.close()\n",
        "\n",
        "      print('_'*80)\n",
        "\n",
        "      # Plot ===================================================================\n",
        "      TEST_PLOT_START = 180\n",
        "      TEST_PLOT_STOP = TEST_PLOT_START + 144*3\n",
        "      days=(TEST_PLOT_STOP-TEST_PLOT_START)//144\n",
        "      print('Plotting results over ', days,' days',sep='')\n",
        "\n",
        "      shift=0#(3-LOOK_FORWARD_DAYS)*260\n",
        "\n",
        "      plt.figure(figsize=(20,10))\n",
        "      plt.subplot(2,2,1)\n",
        "      plt.plot(targets_array[TEST_PLOT_START:TEST_PLOT_STOP], color=\"g\", label=\"Real\", linewidth=0.7)\n",
        "      plt.plot(mm5_array[TEST_PLOT_START:TEST_PLOT_STOP], color=\"b\", label=\"MM5\", linestyle=\"dashed\", linewidth=0.7)\n",
        "      plt.plot(predictions_array_tcn[TEST_PLOT_START:TEST_PLOT_STOP], color=\"r\", label=\"Predicted tcn\", linestyle=\"dashdot\", linewidth=0.7)\n",
        "      plt.ylabel(VARIABLE_NAMES[variable]+' ['+UNITS_OF_MEASUREMENT[variable]+']')\n",
        "      plt.xlabel('Time')\n",
        "      plt.legend()\n",
        "\n",
        "\n",
        "      # Save the figure ========================================================\n",
        "      figure_name = root_tcn + 'figure'+suffix+'.png'\n",
        "      plt.savefig(figure_name, dpi=(250), bbox_inches='tight')\n",
        "\n",
        "\n",
        "    if PLOT_DISTRIBUTIONS:\n",
        "      break\n",
        "\n",
        "  if PLOT_DISTRIBUTIONS:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQVbArUV29-t"
      },
      "outputs": [],
      "source": [
        "if not PLOT_DISTRIBUTIONS:\n",
        "    # Plot =====================================================================\n",
        "    TEST_PLOT_START = 180\n",
        "    TEST_PLOT_STOP = TEST_PLOT_START + 144*3\n",
        "\n",
        "    days=(TEST_PLOT_STOP-TEST_PLOT_START)//144\n",
        "    print('Plotting results over ', days,' days',sep='')\n",
        "\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.subplot(2,2,1)\n",
        "    plt.plot(targets_array[TEST_PLOT_START:TEST_PLOT_STOP], color=\"g\", label=\"Real\", linewidth=0.7)\n",
        "    plt.plot(mm5_array[TEST_PLOT_START:TEST_PLOT_STOP], color=\"b\", label=\"MM5\", linestyle='dashed', linewidth=0.7)\n",
        "    plt.plot(predictions_array_tcn[TEST_PLOT_START:TEST_PLOT_STOP], label=\"TCN Predicted\", linestyle='dashdot', linewidth=0.7)\n",
        "    plt.ylabel(VARIABLE_NAMES[variable])\n",
        "    plt.legend()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}